{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yKmvewGBZ_i"
      },
      "source": [
        "\n",
        "# Graph Attention Network\n",
        "\n",
        "This notebook demonstrates the training of Graph Attention Network (GAT) with TigerGraph. We implement a GAT using the Spektral GNN framework. We train the model on the Cora dataset with TigerGraph as the data store. The dataset contains 2708 machine learning papers and 10556 citation links between the papers. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from a dictionary. The dictionary consists of 1433 unique words. Each paper is classified into one of seven classes based on the topic. The goal is to predict the class of each vertex in the graph.\n",
        "\n",
        "The following libraries are required to run this notebook. Uncomment to install them if necessary. You might need to restart the kernel after installing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah5LXQByBZ_j",
        "outputId": "fc2b47aa-39b3-4498-d498-21484c30e0bd"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow\n",
        "#!pip install spektral\n",
        "#!pip install pyTigerGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "405j7PqyBZ_k"
      },
      "source": [
        "**NOTE**: Currently, your database needs to be activated (only once) to enjoy all the functions provided by the ML Workbench. If you are using ML Workbench on Cloud, then the activator is included and you can run the cell below (uncomment first) to activate. For other versions of the Workbench, you can download the activator at https://act.tigergraphlabs.com. Detailed instructions are also included on that website. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUtDemYKBZ_k"
      },
      "outputs": [],
      "source": [
        "# Uncomment below and fill out the necessary information. For detailed instructions, please see https://act.tigergraphlabs.com\n",
        "# !mlwb activate [database address] -u [username] -p [password] -s [secret]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VIDMjl7BZ_k"
      },
      "source": [
        "## Table of Contents\n",
        "* [Data Processing](#data_processing)  \n",
        "* [Train on whole graph](#train_whole)  \n",
        "* [Train on neighborhood subgraphs](#train_subgraph) \n",
        "* [Inference](#inference) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIv7HAB3BZ_l"
      },
      "source": [
        "## Data Processing <a name=\"data_processing\"></a>\n",
        "\n",
        "Here we assume the dataset is already ingested into the TigerGraph database. If not, please refer to the  [data ingestion](https://github.com/TigerGraph-DevLabs/mlworkbench-docs/blob/main/tutorials/basics/0_data_ingestion.ipynb) tutorial first. Since the dataset already has a split of vertices into train/validation/test sets, we don't need to do so. But we still include the code below for general use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P81VZX1iBZ_l"
      },
      "source": [
        "### Connect to TigerGraph\n",
        "\n",
        "The `TigerGraphConnection` class represents a connection to the TigerGraph database. Under the hood, it stores the necessary information to communicate with the database. It is able to perform quite a few database tasks. Please see its [documentation](https://docs.tigergraph.com/pytigergraph/current/intro/) for details.\n",
        "\n",
        "**Note**: Secret instead of username/password is required for TG cloud DBs created after 7/5/2022. Otherwise, you can leave it blank."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9l7RRfLBZ_l"
      },
      "outputs": [],
      "source": [
        "from pyTigerGraph import TigerGraphConnection\n",
        "\n",
        "conn = TigerGraphConnection(\n",
        "    host=\"http://127.0.0.1\", # Change the address to your database server's\n",
        "    graphname=\"Cora\",\n",
        "    username=\"tigergraph\",\n",
        "    password=\"tigergraph\",\n",
        "    gsqlSecret=\"\" # secret instead of user/pass is required for TG cloud DBs created after 7/5/2022  \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKBEJeMWBZ_m"
      },
      "source": [
        "<span style=\"color:red\">Uncomment cell below and run to get and set token if token authentication is enabled</span>. \n",
        "* This is required for all databases on tgcloud.\n",
        "* `<secret>` is your user secret. See https://docs.tigergraph.com/tigergraph-server/current/user-access/managing-credentials#_secrets for details.\n",
        "* If you don't know your secret, you can use `secret=conn.createSecret()` to create one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxDKUUiIBZ_m"
      },
      "outputs": [],
      "source": [
        "# conn.getToken(<secret>)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ9Rsl1uBZ_n",
        "outputId": "fb686827-f5dd-495d-914b-743565579d95"
      },
      "outputs": [],
      "source": [
        "conn.getVertexCount('*')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEkoNwkDBZ_n",
        "outputId": "dc1b76af-bb2f-4f59-a6af-ef957bfaf7c8"
      },
      "outputs": [],
      "source": [
        "conn.getEdgeCount()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S3jfS2jBZ_o"
      },
      "source": [
        "### Train/validation/test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHS6tn2JBZ_o"
      },
      "outputs": [],
      "source": [
        "# The code in this cell is commented out because there is no need to split the vertices into \n",
        "# training/validation/test sets, as the split is already done in the original dataset. \n",
        "# See notebook 1_data_processing for examples on the split function.\n",
        "\n",
        "#split = conn.gds.vertexSplitter(train_mask=0.8, val_mask=0.1, test_mask=0.1)\n",
        "#split.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvLliqHpBZ_p",
        "outputId": "e370e7f7-8ccd-4957-eafc-6a65abeb5fec"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"Number of vertices in training set:\",\n",
        "    conn.getVertexCount(\"Paper\", where=\"train_mask!=0\"),\n",
        ")\n",
        "print(\n",
        "    \"Number of vertices in validation set:\",\n",
        "    conn.getVertexCount(\"Paper\", where=\"val_mask!=0\"),\n",
        ")\n",
        "print(\n",
        "    \"Number of vertices in test set:\", \n",
        "    conn.getVertexCount(\"Paper\", where=\"test_mask!=0\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mr4C8r8BZ_p"
      },
      "source": [
        "## Train on whole graph <a name=\"train_whole\"></a>\n",
        "We first train the model on the whole graph. This will **NOT** work when the graph is large. See the section of training on subgraphs for real use. However, we still include this example for illustration purpose. Hyperparameters for the model and training environment are defined below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or16LaCSBZ_p"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "hp = {\"hidden_dim\": 64, \n",
        "      \"num_layers\": 2, \n",
        "      \"dropout\": 0.6,\n",
        "      \"lr\": 0.001, \n",
        "      \"l2_penalty\": 5e-4}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbnl3PUnBZ_q"
      },
      "source": [
        "### Construct graph loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k6JP5yYBZ_q"
      },
      "source": [
        "The `GraphLoader` can get the whole graph from database all at once (`num_batches=1`). See the tutorial on dataloaders for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PLb00KaBZ_q"
      },
      "outputs": [],
      "source": [
        "graph_loader = conn.gds.graphLoader(\n",
        "    v_in_feats=[\"x\"],\n",
        "    v_out_labels=[\"y\"],\n",
        "    v_extra_feats=[\"train_mask\", \"val_mask\", \"test_mask\"],\n",
        "    num_batches=1,\n",
        "    output_format=\"spektral\",\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OttlUJHBZ_q",
        "outputId": "3801a54d-2e9c-4d67-f011-0802e6bef8f2"
      },
      "outputs": [],
      "source": [
        "# Get the whole graph from the loader in PyG format\n",
        "data = graph_loader.data\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCpZ3zLUmxTW"
      },
      "outputs": [],
      "source": [
        "x, adj, y, mask_tr, mask_va, mask_te = data.x, data.A, data.y, data.train_mask, data.val_mask, data.test_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUdVtiLGBZ_q"
      },
      "source": [
        "### Construct model and optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17XFCS0QBZ_q"
      },
      "source": [
        "We build a GAT model with 2 convolutional layers, and use the Adam optimizer with a learning rate of 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASrN-H_EBZ_r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from spektral.layers import GATConv\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dropout, Input\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import categorical_accuracy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from spektral.layers import GraphSageConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orl-5y_1BZ_r",
        "outputId": "87f82471-6178-4661-acf9-52f80cef4443"
      },
      "outputs": [],
      "source": [
        "device = tf.device(\"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\")\n",
        "\n",
        "# Model definition\n",
        "l2_reg = hp[\"l2_penalty\"]\n",
        "# Define model\n",
        "x_in = Input(shape=(data.n_node_features,))\n",
        "a_in = Input(shape=(None,), sparse=True)\n",
        "x_1 = Dropout(0.6)(x_in)\n",
        "x_1 = GATConv(\n",
        "    8,\n",
        "    attn_heads=8,\n",
        "    concat_heads=True,\n",
        "    dropout_rate=0.6,\n",
        "    activation=\"elu\",\n",
        "    kernel_regularizer=l2(l2_reg),\n",
        "    attn_kernel_regularizer=l2(l2_reg),\n",
        "    bias_regularizer=l2(l2_reg),\n",
        ")([x_1, a_in])\n",
        "x_2 = Dropout(0.6)(x_1)\n",
        "x_2 = GATConv(\n",
        "    7,\n",
        "    attn_heads=1,\n",
        "    concat_heads=False,\n",
        "    dropout_rate=0.6,\n",
        "    activation=\"softmax\",\n",
        "    kernel_regularizer=l2(l2_reg),\n",
        "    attn_kernel_regularizer=l2(l2_reg),\n",
        "    bias_regularizer=l2(l2_reg),\n",
        ")([x_2, a_in])\n",
        "\n",
        "# Build model\n",
        "model = Model(inputs=[x_in, a_in], outputs=x_2)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4P60Ldknbto"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(learning_rate=hp[\"lr\"])\n",
        "loss_fn = CategoricalCrossentropy()\n",
        "\n",
        "one_hot_y = to_categorical(y)\n",
        "tf_a = tf.SparseTensor(#converts the scipy sparse matrix to a tensorflow sparse matrix\n",
        "    indices=np.array([adj.row, adj.col]).T,\n",
        "    values=adj.data,\n",
        "    dense_shape=adj.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OCi5bF0BZ_s"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ0-OimpBZ_s"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br5-2gCWUVh1"
      },
      "outputs": [],
      "source": [
        "# Training step\n",
        "@tf.function\n",
        "def train():\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model([x, tf_a], training=True)\n",
        "        loss = loss_fn(tf.boolean_mask(one_hot_y, mask_tr), tf.boolean_mask(predictions, mask_tr))\n",
        "        loss += sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXMFhmM8UWBm"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def evaluate():\n",
        "    predictions = model([x, tf_a], training=False)\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    for mask in [mask_tr, mask_va, mask_te]:\n",
        "        loss = loss_fn(tf.boolean_mask(one_hot_y, mask), tf.boolean_mask(predictions, mask))\n",
        "        loss += sum(model.losses)\n",
        "        losses.append(loss)\n",
        "        acc = tf.reduce_mean(categorical_accuracy(tf.boolean_mask(one_hot_y, mask), tf.boolean_mask(predictions, mask)))\n",
        "        accuracies.append(acc)\n",
        "    return losses, accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apXSaEapUYkW",
        "outputId": "ac94a922-755f-4439-999b-73fc24b6e1de"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train()\n",
        "    l, a = evaluate()\n",
        "    print(\n",
        "        \"Epoch {:.0f}:\\n\"\n",
        "        \"Training Loss: {:.4f}, Training Accuracy: {:.4f}, \"\n",
        "        \"Validation Loss: {:.4f}, Validation Accuracy: {:.4f}\".format(epoch, l[0], a[0], l[1], a[1])\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b221-7HaBZ_t"
      },
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOw52gLZVAVK",
        "outputId": "1ff50dc9-6312-4036-a2de-c411a3a37626"
      },
      "outputs": [],
      "source": [
        "l, a = evaluate()\n",
        "\n",
        "print(\"Testing Loss: {:.4f}, Testing Accuracy: {:.4f}\".format(l[2], a[2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afw7ozPEBZ_t"
      },
      "source": [
        "## Train on Neighborhood Subgraphs <a name=\"train_subgraph\"></a>\n",
        "Alternatively, we train the model on the neighborhood subgraphs. Each subgraph contains the 2 hop neighborhood of certain seed vertices. This method  will allow us to train the model on graphs that are way larger than the CORA dataset because we don't load the whole graph into memory all at once. \n",
        "\n",
        "We will use the same parameters as before, but we will use the NeighborLoader to load subgraphs. Once we finish iterating over all the subgraphs generated by the loader, it is guaranteed to cover all vertices in the graph (except for those filtered by a user provided mask). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Eyj0DInBZ_t"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "hp = {\"batch_size\": 64, \n",
        "      \"num_neighbors\": 10, \n",
        "      \"num_hops\": 2, \n",
        "      \"hidden_dim\": 64,\n",
        "      \"num_layers\": 2, \n",
        "      \"dropout\": 0.6, \n",
        "      \"lr\": 0.01, \n",
        "      \"l2_penalty\": 5e-4}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmPjGWx8BZ_t"
      },
      "source": [
        "### Construct neighborhood subgraph loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F-2vvwdBZ_t"
      },
      "source": [
        "Here we construct 3 subgraph loaders. The `train_loader` only uses vertices in the training set as seeds, the `valid_loader` only uses vertices in the validation set, and the `test_loader` only uses vertices in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bslf8ab9BZ_u"
      },
      "outputs": [],
      "source": [
        "train_loader = conn.gds.neighborLoader(\n",
        "    v_in_feats=[\"x\"],\n",
        "    v_out_labels=[\"y\"],\n",
        "    v_extra_feats=[\"train_mask\",\"val_mask\",\"test_mask\"],\n",
        "    output_format=\"spektral\",\n",
        "    batch_size=hp[\"batch_size\"],\n",
        "    num_neighbors=hp[\"num_neighbors\"],\n",
        "    num_hops=hp[\"num_hops\"],\n",
        "    shuffle=True,\n",
        "    filter_by=\"train_mask\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM7HivbNBZ_u"
      },
      "outputs": [],
      "source": [
        "valid_loader = conn.gds.neighborLoader(\n",
        "    v_in_feats=[\"x\"],\n",
        "    v_out_labels=[\"y\"],\n",
        "    v_extra_feats=[\"train_mask\",\"val_mask\",\"test_mask\"],\n",
        "    output_format=\"spektral\",\n",
        "    batch_size=hp[\"batch_size\"],\n",
        "    num_neighbors=hp[\"num_neighbors\"],\n",
        "    num_hops=hp[\"num_hops\"],\n",
        "    shuffle=False,\n",
        "    filter_by=\"val_mask\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qWFudHHBZ_u"
      },
      "source": [
        "### Construct model and optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNX49zi7BZ_u"
      },
      "source": [
        "We build a graphSAGE model with 2 convolutional layers, and use the Adam optimizer with a learning rate of 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD2O9IxGBZ_u",
        "outputId": "dd0a4416-a850-4abf-e9e0-1c8f665e4fbd"
      },
      "outputs": [],
      "source": [
        "model = Model(inputs=[x_in, a_in], outputs=x_2)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vS73lQvVnHJ"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(learning_rate=hp[\"lr\"])\n",
        "loss_fn = CategoricalCrossentropy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo9mZCL8BZ_u"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oboTHGIHBZ_u"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmtxPbLJYhtb"
      },
      "outputs": [],
      "source": [
        "def preprocess_batch(graph):\n",
        "  x, adj, y, mask_tr, mask_va, mask_te = graph.x, graph.A, graph.y, graph.train_mask, graph.val_mask, graph.test_mask\n",
        "  one_hot_y = to_categorical(y)\n",
        "  tf_a = tf.SparseTensor(#converts the scipy sparse matrix to a tensorflow sparse matrix\n",
        "    indices=np.array([adj.row, adj.col]).T,\n",
        "    values=adj.data,\n",
        "    dense_shape=adj.shape)\n",
        "  return x, tf_a, one_hot_y\n",
        "\n",
        "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "val_loss_metric = tf.keras.metrics.CategoricalCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shtEf9eSqaIW"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train(x, tf_a, one_hot_y, mask):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model([x, tf_a], training=True)\n",
        "        loss = loss_fn(tf.boolean_mask(one_hot_y, mask), tf.boolean_mask(predictions, mask))\n",
        "        acc = tf.reduce_mean(categorical_accuracy(tf.boolean_mask(one_hot_y, mask), tf.boolean_mask(predictions, mask)))\n",
        "        loss += sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6KTqhojqbFC"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(x, tf_a, one_hot_y, mask, metrics=[]):\n",
        "    val_logits = model([x, tf_a], training=False)\n",
        "    for metric in metrics:\n",
        "      metric.update_state(tf.boolean_mask(one_hot_y, mask), tf.boolean_mask(val_logits, mask))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC5BkosQBZ_v",
        "outputId": "07881593-0345-4585-ef9e-bd31bee19700"
      },
      "outputs": [],
      "source": [
        "for epoch in range(10):\n",
        "    for bid, batch in enumerate(train_loader):\n",
        "        batchsize = batch.n_nodes\n",
        "        x, tf_a, one_hot_y = preprocess_batch(batch)\n",
        "        loss, acc = train(x, tf_a, one_hot_y, batch.train_mask)\n",
        "        print(\"Epoch {}, Train Batch {}, Loss {:.4f}, Accuracy {:.4f}\".format(epoch, bid, loss, acc))\n",
        "    for batch in valid_loader:\n",
        "        x, tf_a, one_hot_y = preprocess_batch(batch)\n",
        "        test_step(x, tf_a, one_hot_y, batch.val_mask, metrics = [val_acc_metric, val_loss_metric])\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_loss = val_loss_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    val_loss_metric.reset_states()\n",
        "    print(\"Epoch {}, Valid Loss {:.4f}, Valid Accuracy {:.4f}\".format(epoch, val_loss, val_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt56SLbBBZ_w"
      },
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ3uEMjqBZ_w"
      },
      "outputs": [],
      "source": [
        "test_loader = conn.gds.neighborLoader(\n",
        "    v_in_feats=[\"x\"],\n",
        "    v_out_labels=[\"y\"],\n",
        "    v_extra_feats=[\"train_mask\",\"val_mask\",\"test_mask\"],\n",
        "    output_format=\"spektral\",\n",
        "    batch_size=hp[\"batch_size\"],\n",
        "    num_neighbors=hp[\"num_neighbors\"],\n",
        "    num_hops=hp[\"num_hops\"],\n",
        "    shuffle=False,\n",
        "    filter_by=\"test_mask\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80ZVgKH1BZ_w",
        "outputId": "389c2c4a-5ebc-45d9-ea09-1557f5a60a36"
      },
      "outputs": [],
      "source": [
        "acc = tf.keras.metrics.CategoricalAccuracy()\n",
        "for batch in test_loader:\n",
        "    x, tf_a, one_hot_y = preprocess_batch(batch)\n",
        "    test_step(x, tf_a, one_hot_y, batch.val_mask, metrics = [acc])\n",
        "print(\"Accuracy: {:.4f}\".format(acc.result()))\n",
        "acc.reset_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QS2GiqhBZ_w"
      },
      "source": [
        "## Inference <a name=\"inference\"></a>\n",
        "\n",
        "Finally, we use the trained model for node classification. At this stage, we typically do inference/prediction for specific nodes instead of random batches, so we will create a new data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI6p7kGvBZ_w"
      },
      "outputs": [],
      "source": [
        "infer_loader = conn.gds.neighborLoader(\n",
        "    v_in_feats=[\"x\"],\n",
        "    v_out_labels=[\"y\"],\n",
        "    v_extra_feats=[\"train_mask\",\"val_mask\",\"test_mask\"],\n",
        "    output_format=\"spektral\",\n",
        "    num_neighbors=hp[\"num_neighbors\"],\n",
        "    num_hops=hp[\"num_hops\"],\n",
        "    shuffle=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p41oTYfBZ_w"
      },
      "outputs": [],
      "source": [
        "# Fetch specific nodes by their IDs and do prediction. \n",
        "# Each node is represented by a dict with two mandatory keys: primary_id and type.\n",
        "input_nodes = [{\"primary_id\": 7, \"type\": \"Paper\"}, \n",
        "               {\"primary_id\": 999, \"type\": \"Paper\"}]\n",
        "data = infer_loader.fetch(input_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkyHxbzvBZ_w",
        "outputId": "4086d3c9-adcf-4cca-c364-75f571678b55"
      },
      "outputs": [],
      "source": [
        "# The returned data are the neighborhood subgraphs of the input nodes.\n",
        "# The original IDs of the nodes in the subgraphs are stored in the \n",
        "# `primary_id` attribute.\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExEER-TZBZ_w",
        "outputId": "e89effe6-67a1-4c57-f050-ae737747474c"
      },
      "outputs": [],
      "source": [
        "# Predict. Predictions for both the input nodes and others in their \n",
        "# neighborhoods are generated.\n",
        "x, tf_a, one_hot_y = preprocess_batch(batch)\n",
        "pred = model([x, tf_a], training=False)\n",
        "print(\"ID: Label\")\n",
        "for i,j in zip(data.primary_id, pred):\n",
        "    print(\"{}:{}\".format(i, tf.math.argmax(j)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNvMNnOmBZ_w"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GAT Spektral.ipynb",
      "provenance": []
    },
    "environment": {
      "name": "pytorch-gpu.1-9.m81",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m81"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('tigergraph-torch-cpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "fc5eadac82f5951e7eb836bb06f3c9df8e6d1eda5537a95773af6c6ed24cb2d0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
