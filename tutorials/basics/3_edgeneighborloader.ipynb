{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a41fec-c7f0-4752-b57a-efb428327343",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d51c63-ed05-43d4-9e02-7d13763b5011",
   "metadata": {},
   "source": [
    "This notebook demonstrates the use of edge neighbor loader in `pyTigerGraph`. The job of a data loader is to pull data from the TigerGraph database. Currently, the following data loaders are provided:\n",
    "* EdgeLoader, which returns batches of edges.\n",
    "* VertexLoader, which returns batches of vertices.\n",
    "* GraphLoader, which returns randomly sampled (probably disconnected) subgraphs in pandas `dataframe`, `PyG` or `DGL` format.\n",
    "* NeighborLoader, which returns subgraphs using neighbor sampling in `dataframe`, `PyG` or `DGL` format.\n",
    "* EdgeNeighborLoader, which returns subgraphs using neighbor sampling from edges in `dataframe`, `PyG` or `DGL` format.\n",
    "\n",
    "Every data loader above can either get all the batches as a HTTP response (default) or stream every batch through Kafka. The former mechanism is good for testing with small graphs and it is fast, but it subjects to a data size limit of 2GB. For large graphs, the HTTP channel will likely fail due to size limit and network connectivity issues. Streaming via Kafka is offered for data robustness and scalability. Also, Kafka excels at multi-consumer use cases, and it is efficient for model search or hyperparameter tuning when there are multiiple consumers of the same data. \n",
    "\n",
    "The data loaders support both homogeneous and heterogenous graphs. By default, they load from all vertex and edge types and treat the graph as a homogeneous graph. But they also allow users to specify what vertex and edge types to load from and what attributes to load from each type. This way users will get heterogeneous graph outputs.\n",
    "\n",
    "**Note**: For the data loaders to work, a UDF (User Defined Function) has to be installed into the TigerGraph database. It only needs to be installed once per database and will work for all data loaders and all graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9243a4-69ae-4a04-ab82-dc6d393e0cb7",
   "metadata": {},
   "source": [
    "### Connection to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff08e15-5d93-4f30-8a9d-6b101b1604e4",
   "metadata": {},
   "source": [
    "The `TigerGraphConnection` class represents a connection to the TigerGraph database. Under the hood, it stores the necessary information to communicate with the database. It is able to perform quite a few database tasks. Please see its [documentation](https://docs.tigergraph.com/pytigergraph/current/intro/) for details.\n",
    "\n",
    "**Note**: Secret instead of username/password is required for TG cloud DBs created after 7/5/2022. Otherwise, you can leave it blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b94e2237-5050-4c58-91de-c86b804d19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyTigerGraph import TigerGraphConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c4b1e8-a0e2-4026-9bb1-218cdc7ca4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = TigerGraphConnection(\n",
    "    host=\"http://127.0.0.1\", # Change the address to your database server's\n",
    "    graphname=\"Cora\",\n",
    "    username=\"tigergraph\",\n",
    "    password=\"tigergraph\",\n",
    "    gsqlSecret=\"\" # secret instead of user/pass is required for TG cloud DBs created after 7/5/2022  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a3ca46",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Uncomment cell below and run to get and set token if token authentication is enabled</span>. \n",
    "* This is required for all databases on tgcloud.\n",
    "* `<secret>` is your user secret. See https://docs.tigergraph.com/tigergraph-server/current/user-access/managing-credentials#_secrets for details.\n",
    "* If you don't know your secret, you can use `secret=conn.createSecret()` to create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9062559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn.getToken(<secret>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1dd0252-a5e5-47ee-acce-b843571c78cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Paper': 2708}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of vertices for every vertex type\n",
    "conn.getVertexCount('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318f1998-1179-4ae2-9f1b-d1b032076c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cite': 10556}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of edges for every type\n",
    "conn.getEdgeCount()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c20a0-f80b-4720-971a-a81f47929747",
   "metadata": {},
   "source": [
    "Uncomment below to install the UDF. It only needs to be installed once per database and will work for all data loaders and all graphs. **Note**: installing the UDF will overwrite any existing UDF in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7293a660-da94-4ae6-8d57-8f016976b226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExprFunctions installed succesfully\n"
     ]
    }
   ],
   "source": [
    "ExprFunctions=\"https://tg-mlworkbench.s3.us-west-1.amazonaws.com/udf/1.0/ExprFunctions.hpp\"  # For enterprise users, please use the link you received.\n",
    "ExprUtil=\"\"  # For enterprise users, please use the link you received.\n",
    "conn.installUDF(ExprFunctions, ExprUtil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f29ef-6b65-4999-9363-f9dfde5e478c",
   "metadata": {},
   "source": [
    "### Edge Neighbor Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141a8d2-2a4d-4ced-b4ab-17b5e7604994",
   "metadata": {},
   "source": [
    "`EdgeNeighborLoader` performs neighbor sampling from all edges in the graph in batches in the following manner:\n",
    "\n",
    "* It chooses a specified number (`batch_size`) of edges as seeds. The number of batches is the total number of edges divided by the batch size. \n",
    "  * If you specify the number of batches (`num_batches`) instead, `batch_size` is calculated by dividing the total number of edges by the number of batches.\n",
    "  * If specify both parameters, `batch_size` takes priority. \n",
    "* Starting from the vertices attached to the seed edges, it picks a specified number (`num_neighbors`) of neighbors of each vertex at random.\n",
    "* It picks the same number of neighbors for each neighbor, and repeats this process until it finished performing a specified number of hops (`num_hops`).\n",
    "        \n",
    "This generates one subgraph. As you loop through this data loader, every edge will at some point be chosen as a seed and you will get the subgraph expanded from the seeds. If you want to limit seeds to certain edges, the boolean attribute provided to `filter_by` will be used to indicate which edges can be included as seeds. If you want to load from certain types of vertices and edges, \n",
    "use the `dict` input for `v_in_feats`, `v_out_labels`, `v_extra_feats`,\n",
    "`e_in_feats`, `e_out_labels`, `e_extra_feats` where keys of the dict are vertex \n",
    "or edge types to be selected and values are lists of attributes to collect from the\n",
    "vertex or edge types. \n",
    "\n",
    "NOTE: When you initialize the loader on a graph for the first time,\n",
    "the initialization might take a minute as it installs the corresponding\n",
    "query to the database. However, the query installation only\n",
    "needs to be done once, so it will take no time when you initialize the loader\n",
    "on the same graph again.\n",
    "\n",
    "    \n",
    "Args:\n",
    "* v_in_feats (list or dict, optional):\n",
    "        Vertex attributes to be used as input features. \n",
    "        If it is a list, then the attributes\n",
    "        in the list from all vertex types will be selected. An error will be thrown if\n",
    "        certain attribute doesn't exist in all vertex types. If it is a dict, keys of the \n",
    "        dict are vertex types to be selected, and values are lists of attributes to be \n",
    "        selected for each vertex type.\n",
    "        Only numeric and boolean attributes are allowed. The type of an attribute \n",
    "        is automatically determined from the database schema. Defaults to None.\n",
    "* v_out_labels (list or dict, optional):\n",
    "        Vertex attributes to be used as labels for prediction. \n",
    "        If it is a list, then the attributes\n",
    "        in the list from all vertex types will be selected. An error will be thrown if\n",
    "        certain attribute doesn't exist in all vertex types. If it is a dict, keys of the \n",
    "        dict are vertex types to be selected, and values are lists of attributes to be \n",
    "        selected for each vertex type.\n",
    "        Only numeric and boolean attributes are allowed. Defaults to None.\n",
    "* v_extra_feats (list or dict, optional):\n",
    "        Other attributes to get such as indicators of train/test data. \n",
    "        If it is a list, then the attributes\n",
    "        in the list from all vertex types will be selected. An error will be thrown if\n",
    "        certain attribute doesn't exist in all vertex types. If it is a dict, keys of the \n",
    "        dict are vertex types to be selected, and values are lists of attributes to be \n",
    "        selected for each vertex type. \n",
    "        All types of attributes are allowed. Defaults to None.\n",
    "* e_in_feats (list or dict, optional):\n",
    "        Edge attributes to be used as input features. \n",
    "        If it is a list, then the attributes\n",
    "        in the list from all edge types will be selected. An error will be thrown if\n",
    "        certain attribute doesn't exist in all edge types. If it is a dict, keys of the \n",
    "        dict are edge types to be selected, and values are lists of attributes to be \n",
    "        selected for each edge type.\n",
    "        Only numeric and boolean attributes are allowed. The type of an attribute\n",
    "        is automatically determined from the database schema. Defaults to None.\n",
    "* e_out_labels (list or dict, optional):\n",
    "        Edge attributes to be used as labels for prediction. \n",
    "        If it is a list, then the attributes in the list from all edge types will be \n",
    "        selected. An error will be thrown if certain attribute doesn't exist in all \n",
    "        edge types. If it is a dict, keys of the dict are edge types to be selected, \n",
    "        and values are lists of attributes to be selected for each edge type.\n",
    "        Only numeric and boolean attributes are allowed. Defaults to None.\n",
    "* e_extra_feats (list or dict, optional):\n",
    "        Other edge attributes to get such as indicators of train/test data. \n",
    "        If it is a list, then the attributes in the list from all edge types will be \n",
    "        selected. An error will be thrown if certain attribute doesn't exist in all \n",
    "        edge types. If it is a dict, keys of the dict are edge types to be selected, \n",
    "        and values are lists of attributes to be selected for each edge type.\n",
    "        All types of attributes are allowed. Defaults to None.\n",
    "* batch_size (int, optional):  \n",
    "        Number of edges in each batch.  \n",
    "        Defaults to None.  \n",
    "* num_batches (int, optional):  \n",
    "        Number of batches to split the edges.  \n",
    "        Defaults to 1.  \n",
    "* num_neighbors (int, optional):\n",
    "        Number of neighbors to sample for each vertex.\n",
    "        Defaults to 10.\n",
    "* num_hops (int, optional):\n",
    "        Number of hops to traverse when sampling neighbors.\n",
    "        Defaults to 2.\n",
    "* shuffle (bool, optional):  \n",
    "        Whether to shuffle the edges before loading data.  \n",
    "        Defaults to False.  \n",
    "* filter_by (str, optional):\n",
    "        A boolean attribute used to indicate which edges are included. Defaults to None.\n",
    "* output_format (str, optional):\n",
    "        Format of the output data of the loader. Only\n",
    "        \"dataframe\" is supported. Defaults to \"dataframe\".\n",
    "* loader_id (str, optional):\n",
    "        An identifier of the loader which can be any string. It is\n",
    "        also used as the Kafka topic name. If `None`, a random string will be generated\n",
    "        for it. Defaults to None.\n",
    "* buffer_size (int, optional):\n",
    "        Number of data batches to prefetch and store in memory. Defaults to 4.\n",
    "* kafka_address (str, optional):\n",
    "        Address of the kafka broker. Defaults to None.\n",
    "* kafka_max_msg_size (int, optional):\n",
    "        Maximum size of a Kafka message in bytes.\n",
    "        Defaults to 104857600.\n",
    "* kafka_num_partitions (int, optional):\n",
    "        Number of partitions for the topic created by this loader.\n",
    "        Defaults to 1.\n",
    "* kafka_replica_factor (int, optional):\n",
    "        Number of replications for the topic created by this\n",
    "        loader. Defaults to 1.\n",
    "* kafka_retention_ms (int, optional):\n",
    "        Retention time for messages in the topic created by this\n",
    "        loader in milliseconds. Defaults to 60000.\n",
    "* kafka_auto_del_topic (bool, optional):\n",
    "        Whether to delete the Kafka topic once the\n",
    "        loader finishes pulling data. Defaults to True.\n",
    "* kafka_address_consumer (str, optional):\n",
    "        Address of the kafka broker that a consumer\n",
    "        should use. Defaults to be the same as `kafkaAddress`.\n",
    "* kafka_address_producer (str, optional):\n",
    "        Address of the kafka broker that a producer\n",
    "        should use. Defaults to be the same as `kafkaAddress`.\n",
    "* timeout (int, optional):\n",
    "        Timeout value for GSQL queries, in ms. Defaults to 300000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc6fb4-9e7b-4d7a-8c97-aa5d43b18468",
   "metadata": {},
   "source": [
    "#### Get subgraphs through http"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "269bf653-ea3a-4646-aa0d-ef539838a7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing and optimizing queries. It might take a minute if this is the first time you use this loader.\n",
      "Query installation finished.\n",
      "CPU times: user 213 ms, sys: 42.3 ms, total: 255 ms\n",
      "Wall time: 45.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "neighbor_loader = conn.gds.edgeNeighborLoader(\n",
    "    num_batches=10,\n",
    "    num_neighbors = 10,\n",
    "    num_hops =2,\n",
    "    v_in_feats = [\"x\"],\n",
    "    v_out_labels = [\"y\"],\n",
    "    v_extra_feats = [\"train_mask\", \"val_mask\", \"test_mask\"],\n",
    "    e_in_feats=[\"time\"],\n",
    "    e_out_labels=[],\n",
    "    e_extra_feats=[\"is_train\", \"is_val\"],\n",
    "    output_format = \"PyG\",\n",
    "    shuffle=False,\n",
    "    filter_by=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc82350c-9e80-4ff1-8c1b-67ae60a75a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Batch 0----\n",
      "Data(edge_index=[2, 7995], edge_feat=[7995], is_train=[7995], is_val=[7995], is_seed=[7995], x=[2337, 1433], y=[2337], train_mask=[2337], val_mask=[2337], test_mask=[2337])\n",
      "----Batch 1----\n",
      "Data(edge_index=[2, 8560], edge_feat=[8560], is_train=[8560], is_val=[8560], is_seed=[8560], x=[2435, 1433], y=[2435], train_mask=[2435], val_mask=[2435], test_mask=[2435])\n",
      "----Batch 2----\n",
      "Data(edge_index=[2, 8774], edge_feat=[8774], is_train=[8774], is_val=[8774], is_seed=[8774], x=[2432, 1433], y=[2432], train_mask=[2432], val_mask=[2432], test_mask=[2432])\n",
      "----Batch 3----\n",
      "Data(edge_index=[2, 8855], edge_feat=[8855], is_train=[8855], is_val=[8855], is_seed=[8855], x=[2450, 1433], y=[2450], train_mask=[2450], val_mask=[2450], test_mask=[2450])\n",
      "----Batch 4----\n",
      "Data(edge_index=[2, 8574], edge_feat=[8574], is_train=[8574], is_val=[8574], is_seed=[8574], x=[2428, 1433], y=[2428], train_mask=[2428], val_mask=[2428], test_mask=[2428])\n",
      "----Batch 5----\n",
      "Data(edge_index=[2, 7940], edge_feat=[7940], is_train=[7940], is_val=[7940], is_seed=[7940], x=[2343, 1433], y=[2343], train_mask=[2343], val_mask=[2343], test_mask=[2343])\n",
      "----Batch 6----\n",
      "Data(edge_index=[2, 8776], edge_feat=[8776], is_train=[8776], is_val=[8776], is_seed=[8776], x=[2464, 1433], y=[2464], train_mask=[2464], val_mask=[2464], test_mask=[2464])\n",
      "----Batch 7----\n",
      "Data(edge_index=[2, 8882], edge_feat=[8882], is_train=[8882], is_val=[8882], is_seed=[8882], x=[2449, 1433], y=[2449], train_mask=[2449], val_mask=[2449], test_mask=[2449])\n",
      "----Batch 8----\n",
      "Data(edge_index=[2, 8964], edge_feat=[8964], is_train=[8964], is_val=[8964], is_seed=[8964], x=[2483, 1433], y=[2483], train_mask=[2483], val_mask=[2483], test_mask=[2483])\n",
      "----Batch 9----\n",
      "Data(edge_index=[2, 8591], edge_feat=[8591], is_train=[8591], is_val=[8591], is_seed=[8591], x=[2398, 1433], y=[2398], train_mask=[2398], val_mask=[2398], test_mask=[2398])\n",
      "CPU times: user 26.4 s, sys: 1.14 s, total: 27.5 s\n",
      "Wall time: 9.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, batch in enumerate(neighbor_loader):\n",
    "    print(\"----Batch {}----\".format(i))\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff561ba1",
   "metadata": {},
   "source": [
    "#### For heterogeneous graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dee177",
   "metadata": {},
   "source": [
    "Since `Cora` is a homogeneous graph, we will connect to a different graph to demostrate the use case of heterogeneous graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29498931",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = TigerGraphConnection(\n",
    "    host=\"http://127.0.0.1\", # Change the address to your database server's\n",
    "    graphname=\"hetero\",\n",
    "    username=\"tigergraph\",\n",
    "    password=\"tigergraph\",\n",
    "    gsqlSecret=\"\" # secret instead of user/pass is required for TG cloud DBs created after 7/5/2022  \n",
    ")\n",
    "\n",
    "# Uncomment below to get and set token if token authentication is enabled. \n",
    "#conn.getToken(<secret>) # <secret> is your user secret. See https://docs.tigergraph.com/tigergraph-server/current/user-access/managing-credentials#_secrets for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19ea6bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Graph hetero\n",
      "Vertex Types:\n",
      "- VERTEX v0(PRIMARY_ID id INT, x LIST<DOUBLE>, y INT, train_mask BOOL, val_mask BOOL, test_mask BOOL) WITH STATS=\"OUTDEGREE_BY_EDGETYPE\", PRIMARY_ID_AS_ATTRIBUTE=\"true\"\n",
      "- VERTEX v1(PRIMARY_ID id INT, x LIST<DOUBLE>, train_mask BOOL, val_mask BOOL, test_mask BOOL) WITH STATS=\"OUTDEGREE_BY_EDGETYPE\", PRIMARY_ID_AS_ATTRIBUTE=\"true\"\n",
      "- VERTEX v2(PRIMARY_ID id INT, x LIST<DOUBLE>, train_mask BOOL, val_mask BOOL, test_mask BOOL) WITH STATS=\"OUTDEGREE_BY_EDGETYPE\", PRIMARY_ID_AS_ATTRIBUTE=\"true\"\n",
      "Edge Types:\n",
      "- DIRECTED EDGE v0v0(FROM v0, TO v0, is_train BOOL, is_val BOOL)\n",
      "- DIRECTED EDGE v1v1(FROM v1, TO v1, is_train BOOL, is_val BOOL)\n",
      "- DIRECTED EDGE v1v2(FROM v1, TO v2, is_train BOOL, is_val BOOL)\n",
      "- DIRECTED EDGE v2v0(FROM v2, TO v0, is_train BOOL, is_val BOOL)\n",
      "- DIRECTED EDGE v2v1(FROM v2, TO v1, is_train BOOL, is_val BOOL)\n",
      "- DIRECTED EDGE v2v2(FROM v2, TO v2, is_train BOOL, is_val BOOL)\n",
      "\n",
      "Graphs:\n",
      "- Graph hetero(v0:v, v1:v, v2:v, v0v0:e, v1v1:e, v1v2:e, v2v0:e, v2v1:e, v2v2:e)\n",
      "Jobs:\n",
      "- CREATE LOADING JOB load_hetero_data FOR GRAPH hetero {\n",
      "DEFINE FILENAME v2v0_csv = \"./v2v0.csv\";\n",
      "DEFINE FILENAME v2v1_csv = \"./v2v1.csv\";\n",
      "DEFINE FILENAME v2v2_csv = \"./v2v2.csv\";\n",
      "DEFINE FILENAME v1_csv = \"./v1.csv\";\n",
      "DEFINE FILENAME v0v0_csv = \"./v0v0.csv\";\n",
      "DEFINE FILENAME v2_csv = \"./v2.csv\";\n",
      "DEFINE FILENAME v1v1_csv = \"./v1v1.csv\";\n",
      "DEFINE FILENAME v1v2_csv = \"./v1v2.csv\";\n",
      "DEFINE FILENAME v0_csv = \"./v0.csv\";\n",
      "LOAD v0_csv TO VERTEX v0 VALUES($\"id\", SPLIT($\"x\", \" \"), $\"y\", _, _, _) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\";\n",
      "LOAD v1_csv TO VERTEX v1 VALUES($\"id\", SPLIT($\"x\", \" \"), _, _, _) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\";\n",
      "LOAD v2_csv TO VERTEX v2 VALUES($\"id\", SPLIT($\"x\", \" \"), _, _, _) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\";\n",
      "LOAD v0v0_csv TO EDGE v0v0 VALUES($\"source\", $\"target\", _, _) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\";\n",
      "LOAD v1v1_csv TO EDGE v1v1 VALUES($\"source\", $\"target\", _, _) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\";\n",
      "LOAD v1v2_csv TO EDGE v1v2 VALUES($\"source\", $\"target\", _, _) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\";\n",
      "LOAD v2v0_csv TO EDGE v2v0 VALUES($\"source\", $\"target\", _, _) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\";\n",
      "LOAD v2v1_csv TO EDGE v2v1 VALUES($\"source\", $\"target\", _, _) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\";\n",
      "LOAD v2v2_csv TO EDGE v2v2 VALUES($\"source\", $\"target\", _, _) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\n\";\n",
      "}\n",
      "\n",
      "- CREATE SCHEMA_CHANGE JOB hetero_job FOR GRAPH hetero {\n",
      "ADD VERTEX v0(PRIMARY_ID id INT, x LIST<DOUBLE>, y INT, train_mask BOOL, val_mask BOOL, test_mask BOOL) WITH STATS=\"OUTDEGREE_BY_EDGETYPE\", PRIMARY_ID_AS_ATTRIBUTE=\"true\";\n",
      "ADD VERTEX v1(PRIMARY_ID id INT, x LIST<DOUBLE>, train_mask BOOL, val_mask BOOL, test_mask BOOL) WITH STATS=\"OUTDEGREE_BY_EDGETYPE\", PRIMARY_ID_AS_ATTRIBUTE=\"true\";\n",
      "ADD VERTEX v2(PRIMARY_ID id INT, x LIST<DOUBLE>, train_mask BOOL, val_mask BOOL, test_mask BOOL) WITH STATS=\"OUTDEGREE_BY_EDGETYPE\", PRIMARY_ID_AS_ATTRIBUTE=\"true\";\n",
      "ADD DIRECTED EDGE v0v0(FROM v0, TO v0, is_train BOOL, is_val BOOL);\n",
      "ADD DIRECTED EDGE v1v1(FROM v1, TO v1, is_train BOOL, is_val BOOL);\n",
      "ADD DIRECTED EDGE v1v2(FROM v1, TO v2, is_train BOOL, is_val BOOL);\n",
      "ADD DIRECTED EDGE v2v0(FROM v2, TO v0, is_train BOOL, is_val BOOL);\n",
      "ADD DIRECTED EDGE v2v1(FROM v2, TO v1, is_train BOOL, is_val BOOL);\n",
      "ADD DIRECTED EDGE v2v2(FROM v2, TO v2, is_train BOOL, is_val BOOL);\n",
      "}\n",
      "\n",
      "Queries:\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(conn.gsql(\"ls\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61528927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing and optimizing queries. It might take a minute if this is the first time you use this loader.\n",
      "Query installation finished.\n",
      "CPU times: user 26.8 ms, sys: 5.07 ms, total: 31.8 ms\n",
      "Wall time: 45.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "neighbor_loader = conn.gds.edgeNeighborLoader(\n",
    "    v_in_feats={\"v0\": [\"x\"],\n",
    "                \"v1\": [\"x\"],\n",
    "                \"v2\": [\"x\"]},\n",
    "    v_out_labels={\"v0\": [\"y\"]},\n",
    "    v_extra_feats={\"v0\": [\"train_mask\", \"val_mask\", \"test_mask\"]},\n",
    "    e_extra_feats={\"v0v0\": [\"is_train\"], \n",
    "                   \"v2v0\": [\"is_train\"],\n",
    "                   \"v1v2\": [\"is_train\"]},\n",
    "    num_batches=16,\n",
    "    num_neighbors=10,\n",
    "    num_hops=2,\n",
    "    shuffle=False,\n",
    "    output_format=\"PyG\",\n",
    "    add_self_loop=False,\n",
    "    loader_id=None,\n",
    "    buffer_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47b95b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Batch 0----\n",
      "HeteroData(\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1mv1\u001b[0m={ x=[46, 57] },\n",
      "  \u001b[1mv2\u001b[0m={ x=[100, 48] },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 663],\n",
      "    is_train=[663],\n",
      "    is_seed=[663]\n",
      "  },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 400],\n",
      "    is_train=[400],\n",
      "    is_seed=[400]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 884],\n",
      "    is_train=[884],\n",
      "    is_seed=[884]\n",
      "  }\n",
      ")\n",
      "----Batch 1----\n",
      "HeteroData(\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1mv1\u001b[0m={ x=[47, 57] },\n",
      "  \u001b[1mv2\u001b[0m={ x=[99, 48] },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 408],\n",
      "    is_train=[408],\n",
      "    is_seed=[408]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 872],\n",
      "    is_train=[872],\n",
      "    is_seed=[872]\n",
      "  },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 655],\n",
      "    is_train=[655],\n",
      "    is_seed=[655]\n",
      "  }\n",
      ")\n",
      "----Batch 2----\n",
      "HeteroData(\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1mv1\u001b[0m={ x=[40, 57] },\n",
      "  \u001b[1mv2\u001b[0m={ x=[99, 48] },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 366],\n",
      "    is_train=[366],\n",
      "    is_seed=[366]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 875],\n",
      "    is_train=[875],\n",
      "    is_seed=[875]\n",
      "  },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 671],\n",
      "    is_train=[671],\n",
      "    is_seed=[671]\n",
      "  }\n",
      ")\n",
      "----Batch 3----\n",
      "HeteroData(\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1mv1\u001b[0m={ x=[37, 57] },\n",
      "  \u001b[1mv2\u001b[0m={ x=[98, 48] },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 660],\n",
      "    is_train=[660],\n",
      "    is_seed=[660]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 866],\n",
      "    is_train=[866],\n",
      "    is_seed=[866]\n",
      "  },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 339],\n",
      "    is_train=[339],\n",
      "    is_seed=[339]\n",
      "  }\n",
      ")\n",
      "----Batch 4----\n",
      "HeteroData(\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1mv1\u001b[0m={ x=[40, 57] },\n",
      "  \u001b[1mv2\u001b[0m={ x=[96, 48] },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 651],\n",
      "    is_train=[651],\n",
      "    is_seed=[651]\n",
      "  },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 367],\n",
      "    is_train=[367],\n",
      "    is_seed=[367]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 849],\n",
      "    is_train=[849],\n",
      "    is_seed=[849]\n",
      "  }\n",
      ")\n",
      "----Batch 5----\n",
      "HeteroData(\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1mv1\u001b[0m={ x=[23, 57] },\n",
      "  \u001b[1mv2\u001b[0m={ x=[91, 48] },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 654],\n",
      "    is_train=[654],\n",
      "    is_seed=[654]\n",
      "  },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 205],\n",
      "    is_train=[205],\n",
      "    is_seed=[205]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 793],\n",
      "    is_train=[793],\n",
      "    is_seed=[793]\n",
      "  }\n",
      ")\n",
      "----Batch 6----\n",
      "HeteroData(\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1mv1\u001b[0m={ x=[18, 57] },\n",
      "  \u001b[1mv2\u001b[0m={ x=[84, 48] },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 159],\n",
      "    is_train=[159],\n",
      "    is_seed=[159]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 735],\n",
      "    is_train=[735],\n",
      "    is_seed=[735]\n",
      "  },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 653],\n",
      "    is_train=[653],\n",
      "    is_seed=[653]\n",
      "  }\n",
      ")\n",
      "----Batch 7----\n",
      "HeteroData(\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1mv1\u001b[0m={ x=[32, 57] },\n",
      "  \u001b[1mv2\u001b[0m={ x=[97, 48] },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 658],\n",
      "    is_train=[658],\n",
      "    is_seed=[658]\n",
      "  },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 277],\n",
      "    is_train=[277],\n",
      "    is_seed=[277]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 853],\n",
      "    is_train=[853],\n",
      "    is_seed=[853]\n",
      "  }\n",
      ")\n",
      "----Batch 8----\n",
      "HeteroData(\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1mv1\u001b[0m={ x=[39, 57] },\n",
      "  \u001b[1mv2\u001b[0m={ x=[98, 48] },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 655],\n",
      "    is_train=[655],\n",
      "    is_seed=[655]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 862],\n",
      "    is_train=[862],\n",
      "    is_seed=[862]\n",
      "  },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 337],\n",
      "    is_train=[337],\n",
      "    is_seed=[337]\n",
      "  }\n",
      ")\n",
      "----Batch 9----\n",
      "HeteroData(\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1mv1\u001b[0m={ x=[35, 57] },\n",
      "  \u001b[1mv2\u001b[0m={ x=[97, 48] },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 309],\n",
      "    is_train=[309],\n",
      "    is_seed=[309]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 855],\n",
      "    is_train=[855],\n",
      "    is_seed=[855]\n",
      "  },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 653],\n",
      "    is_train=[653],\n",
      "    is_seed=[653]\n",
      "  }\n",
      ")\n",
      "----Batch 10----\n",
      "HeteroData(\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1mv2\u001b[0m={ x=[84, 48] },\n",
      "  \u001b[1mv1\u001b[0m={ x=[19, 57] },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 175],\n",
      "    is_train=[175],\n",
      "    is_seed=[175]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 739],\n",
      "    is_train=[739],\n",
      "    is_seed=[739]\n",
      "  },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 655],\n",
      "    is_train=[655],\n",
      "    is_seed=[655]\n",
      "  }\n",
      ")\n",
      "----Batch 11----\n",
      "HeteroData(\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1mv1\u001b[0m={ x=[11, 57] },\n",
      "  \u001b[1mv2\u001b[0m={ x=[66, 48] },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 654],\n",
      "    is_train=[654],\n",
      "    is_seed=[654]\n",
      "  },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 104],\n",
      "    is_train=[104],\n",
      "    is_seed=[104]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 573],\n",
      "    is_train=[573],\n",
      "    is_seed=[573]\n",
      "  }\n",
      ")\n",
      "----Batch 12----\n",
      "HeteroData(\n",
      "  \u001b[1mv1\u001b[0m={ x=[24, 57] },\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1mv2\u001b[0m={ x=[97, 48] },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 218],\n",
      "    is_train=[218],\n",
      "    is_seed=[218]\n",
      "  },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 646],\n",
      "    is_train=[646],\n",
      "    is_seed=[646]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 858],\n",
      "    is_train=[858],\n",
      "    is_seed=[858]\n",
      "  }\n",
      ")\n",
      "----Batch 13----\n",
      "HeteroData(\n",
      "  \u001b[1mv1\u001b[0m={ x=[22, 57] },\n",
      "  \u001b[1mv2\u001b[0m={ x=[86, 48] },\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 199],\n",
      "    is_train=[199],\n",
      "    is_seed=[199]\n",
      "  },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 653],\n",
      "    is_train=[653],\n",
      "    is_seed=[653]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 755],\n",
      "    is_train=[755],\n",
      "    is_seed=[755]\n",
      "  }\n",
      ")\n",
      "----Batch 14----\n",
      "HeteroData(\n",
      "  \u001b[1mv1\u001b[0m={ x=[20, 57] },\n",
      "  \u001b[1mv2\u001b[0m={ x=[83, 48] },\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 653],\n",
      "    is_train=[653],\n",
      "    is_seed=[653]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 729],\n",
      "    is_train=[729],\n",
      "    is_seed=[729]\n",
      "  },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 176],\n",
      "    is_train=[176],\n",
      "    is_seed=[176]\n",
      "  }\n",
      ")\n",
      "----Batch 15----\n",
      "HeteroData(\n",
      "  \u001b[1mv0\u001b[0m={\n",
      "    x=[76, 77],\n",
      "    y=[76],\n",
      "    train_mask=[76],\n",
      "    val_mask=[76],\n",
      "    test_mask=[76]\n",
      "  },\n",
      "  \u001b[1mv1\u001b[0m={ x=[11, 57] },\n",
      "  \u001b[1mv2\u001b[0m={ x=[68, 48] },\n",
      "  \u001b[1m(v0, v0v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 653],\n",
      "    is_train=[653],\n",
      "    is_seed=[653]\n",
      "  },\n",
      "  \u001b[1m(v2, v2v0, v0)\u001b[0m={\n",
      "    edge_index=[2, 594],\n",
      "    is_train=[594],\n",
      "    is_seed=[594]\n",
      "  },\n",
      "  \u001b[1m(v1, v1v2, v2)\u001b[0m={\n",
      "    edge_index=[2, 103],\n",
      "    is_train=[103],\n",
      "    is_seed=[103]\n",
      "  }\n",
      ")\n",
      "CPU times: user 656 ms, sys: 20.8 ms, total: 677 ms\n",
      "Wall time: 871 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, batch in enumerate(neighbor_loader):\n",
    "    print(\"----Batch {}----\".format(i))\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8dd1f1-9139-4f7a-b10f-3ecd60226541",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Streaming through Kafka\n",
    "\n",
    "**Note**: Kafka streaming is only available for the Enterprise Edition. You need to install the UDF with Kafka support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ea2e8-4f17-4b63-ae4e-aecd938d652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ExprFunctions=\"./ExprFunctions.hpp\"  # For enterprise users, please use the link you received.\n",
    "ExprUtil=\"./ExprUtil.hpp\"  # For enterprise users, please use the link you received.\n",
    "conn.installUDF(ExprFunctions, ExprUtil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ad477-db09-4e4a-9cdd-e282753b11d5",
   "metadata": {},
   "source": [
    "#### Configure Kafka\n",
    "Set up Kafka here. Once configured, the settings will be shared with all newly created data loaders and no need to set up Kafka for each loader. Please see official https://docs.tigergraph.com/pytigergraph/current/gds/gds for detailed settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c4735-1c40-4bca-87b5-dd437c4a0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.gds.configureKafka(\n",
    "    kafka_address=\"127.0.0.1:9092\",\n",
    "    kafka_security_protocol=\"SASL_PLAINTEXT\",\n",
    "    kafka_sasl_mechanism=\"PLAIN\",\n",
    "    kafka_sasl_plain_username=\"your username\",\n",
    "    kafka_sasl_plain_password=\"your password\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12658c2-4521-48cc-8e6b-79ef7b064cf9",
   "metadata": {},
   "source": [
    "#### Get subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a869d-6b10-4f48-8fae-adc757e757cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "neighbor_loader = conn.gds.EdgeNeighborLoader(\n",
    "    num_batches=10,\n",
    "    num_neighbors = 10,\n",
    "    num_hops =2,\n",
    "    v_in_feats = [\"x\"],\n",
    "    v_out_labels = [\"y\"],\n",
    "    v_extra_feats = [\"train_mask\", \"val_mask\", \"test_mask\"],\n",
    "    e_in_feats=[\"time\"],\n",
    "    e_out_labels=[],\n",
    "    e_extra_feats=[\"is_train\", \"is_val\"],\n",
    "    output_format = \"PyG\",\n",
    "    shuffle=True,\n",
    "    filter_by=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375539a-7b5c-4fe9-ab3e-53a78c9a2845",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, batch in enumerate(neighbor_loader):\n",
    "    print(\"----Batch {}----\".format(i))\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b885f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m81"
  },
  "interpreter": {
   "hash": "96daeecb52bbbb8e3aef04d2f9c6a1e01f271d07cea30059f3c558ef00b717d2"
  },
  "kernelspec": {
   "display_name": "TigerGraph Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
